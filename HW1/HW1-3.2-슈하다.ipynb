{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPflxUcsEV0PzyLcF0Tqnl0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##3.2 Object-Oriented Design for Implementation"],"metadata":{"id":"iwY-mSR-ksKF"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"B-tpInyxkn-4"},"outputs":[],"source":["pip install d2l"]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"metadata":{"id":"eq3OesBflHIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.2.1 Utilities"],"metadata":{"id":"_oI50sKHlhLr"}},{"cell_type":"code","source":["def add_to_class(Class): #save\n","  \"\"\"Register functions as methods in created class.\"\"\"\n","  def wrapper(obj):\n","    setattr(Class, obj.__name__, obj)\n","  return wrapper"],"metadata":{"id":"8fsQJdKlletF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class A:\n","  def __init__(self):\n","    self.b= 1\n","\n","a = A()"],"metadata":{"id":"edJO8DpKmGgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@add_to_class(A)\n","def do(self):\n","  print('Class attribute \"b\" is', self.b)\n","\n","a.do()"],"metadata":{"id":"JUdHh5ndmSt8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HyperParameters: #save\n","  \"\"\"The base class of hyperparameters.\"\"\"\n","  def save_hyperparameters(self, ignore=[]):\n","    raise NotImplemented"],"metadata":{"id":"cywtqbTHmpXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the fully implemented HyperParameters class saved iind2l\n","class B(d2l.HyperParameters):\n","  def __init__(self, a, b, c):\n","    self.save_hyperparameters(ignore=['c'])\n","    print('self.a =', self.a, 'self.b =', self.b)\n","    print('There is no self.c =', not hasattr(self, 'c'))\n","\n","b = B(a=1, b=2, c=3)"],"metadata":{"id":"86SRACsPm-m2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ProgressBoard(d2l.HyperParameters): #save\n","  \"\"\"The board that plots data points in animation.\"\"\"\n","  def __init__(self, xlabel=None, ylabel=None, xlim=None,\n","               ylim=None, xscale='linear', yscale='linear',\n","               ls=['-', '--', '-.', ':'], colors=['C0', 'C1', 'C2', 'C3'],\n","               fig = None, axes = None, figsize=(3.5, 2.5), display=True):\n","    self.save_hyperparameters()\n","\n","  def draw(self, x, y, label, every_n=1):\n","    raise NotImplemented"],"metadata":{"id":"rofc8kXznfA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["board = d2l.ProgressBoard('x')\n","for x in np.arange(0, 10, 0.1):\n","  board.draw(x, np.sin(x), 'sin', every_n=2)\n","  board.draw(x, np.cos(x), 'cos', every_n=10)"],"metadata":{"id":"cbJwxKu3om7t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.2.2 Models"],"metadata":{"id":"YJxRJx5tpClF"}},{"cell_type":"code","source":["class Module(nn.Module, d2l.HyperParameters): #save\n","  \"\"\"The base class of models.\"\"\"\n","  def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n","    super().__init__()\n","    self.save_hyperparameters()\n","    self.board = ProgressBoard()\n","\n","  def loss(self, y_hat, y):\n","    raise ImplementError\n","\n","  def forward(self, x):\n","    assert hasattr(self, 'net'), 'Neural network is defined'\n","    return self.net(X)\n","\n","  def plot(self, key, value, train):\n","    \"\"\"Plot a point in animation.\"\"\"\n","    assert hasattr(self, 'trainer'), 'Trainer is not inited'\n","    self.board.xlabel = 'epoch'\n","    if train:\n","      x = self.trainer.train_batch_idex / \\\n","          self.trainer.num_train_batches\n","      n = self.trainer.num_train_batches / \\\n","          self.plot_train_per_epoch\n","\n","    else:\n","      x = self.trainer.epoch + 1\n","      n = self.trainer.num_val_batches / \\\n","          self.plot_valid_per_epoch\n","    self.board.draw(X, value.to(d2l.cpu()).detach().numpy(),\n","                    ('train_' if train else 'val_') + key,\n","                    every_n =  int(n))\n","\n","  def training_step(self, batch):\n","    l = self.loss(self(*batch[:-1]), batch[-1])\n","    self.plot('loss', l, train = True)\n","    return l\n","\n","  def validation_step(self, batch):\n","    l = self.loss(self(*batch[:-1]), batch[-1])\n","    self.plot('loss', l, rain=False)\n","\n","  def configure_optimizers(self):\n","    raise NotImplementedError"],"metadata":{"id":"ST69_L93o5cb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.2.3 Data"],"metadata":{"id":"HMq1Vca-re-z"}},{"cell_type":"code","source":["class fataModule(d2l.HyperParameters): #save\n","  \"\"\"\"The base class of data.\"\"\"\n","  def __init__(self, root='../data', num_workers = 4):\n","    self.save_hyperparameters()\n","\n","  def get_dataloader(self_train):\n","    raise NotImplementedError\n","\n","  def train_dataloader(self):\n","    return self.get_dataloader(train=True)\n","\n","  def val_dataloader(self):\n","    return self.get_dataloader(train=False)"],"metadata":{"id":"NXITSWqhrbC4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.2.4 Training"],"metadata":{"id":"KkiWZcJJsOoV"}},{"cell_type":"code","source":["class Trainer(d2l.HyperParameters): #save\n","  \"\"\"The base class for training model with data.\"\"\"\n","  def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n","    self.save_hyperparameters()\n","    assert num_gpus == 0, 'No GPU support yet'\n","\n","  def prepare_data(self, data):\n","    self.train_dataloader = data.train_dataloader()\n","    self.val_dataloader = data.val_dataloader()\n","    self.num_train_batches = len(self.train_dataloader)\n","    self.num_val_batches = (len(self.val_dataloader)\n","                            if self.val_dataloader is not None else 0)\n","\n","  def prepare_model(self, model):\n","    model.trainer = self\n","    model.board.xlim = [0, self.max_epochs]\n","    self.model = model\n","\n","  def fit(self, model, data):\n","    self.prepare_data(data)\n","    self.prepare_model(model)\n","    self.optim = model.configure_optimizers()\n","    self.epoch = 0\n","    self.train_batch_idex = 0\n","    self.val_batch_idx = 0\n","    for self.epoch in range(self.max_epochs):\n","      self.fit_epoch()\n","\n","  def fit_epoch(self):\n","    raise NotImplementedError"],"metadata":{"id":"G-CD7dTNsJyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Discussion and Review 3.2"],"metadata":{"id":"bzWhwBEluRNR"}},{"cell_type":"markdown","source":["###3.2.2\n","\n","Key Features of the Module Class\n","\n","- Inheritance from `nn.Module`: Ensures compatibility with PyTorch, leveraging its built-in functionalities for `forward()` calls and automatic parameter registration.\n","- Loss Computation and Visualization: The `training_step` and `validation_step` methods manage loss calculations and interactive plotting.\n","- Optimizer Configuration: The `configure_optimizers` method is designed to be implemented in specific models to specify the optimization algorithms.\n"],"metadata":{"id":"meJr0sJzGKch"}},{"cell_type":"markdown","source":["###3.2.3\n","\n","Key Features of the Trainer Class\n","\n","- Data Preparation: The `prepare_data` method sets up data loaders for both training and validation.\n","- Model Preparation: The `prepare_model `method links the model to the trainer, configuring plotting parameters.\n","- Training Loop: The `fit` method orchestrates the training process over a specified number of epochs, coordinating data and model interactions.\n"],"metadata":{"id":"vslxvyiBGeZ1"}},{"cell_type":"markdown","source":["##3.4 Linear Regression Implementation from Scratch"],"metadata":{"id":"LDo41Zy8mwz8"}},{"cell_type":"code","source":["%matplotlib inline\n","import torch\n","from d2l import torch as d2l"],"metadata":{"id":"yBDrNjyPm7go"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.4.1 Defining the Model"],"metadata":{"id":"i4p7TgEQm3oZ"}},{"cell_type":"code","source":["class LinearRegressionScratch(d2l.Module): #save\n","  \"\"\"The linear regression model implemented from scratch.\"\"\"\n","  def __init__(self, num_inputs, lr, sigma=0.01):\n","    super().__init__()\n","    self.save_hyperparameters()\n","    self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n","    self.b = torch.zeros(1, requires_grad=True)"],"metadata":{"id":"2oOKrEpKnXU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(LinearRegressionScratch) #save\n","def forward(self, X):\n","  return torch.matmul(X, self.w) + self.b"],"metadata":{"id":"HtAHsNt0oXeF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.4.2 Defining the Loss Function"],"metadata":{"id":"dM1sw8j5osx5"}},{"cell_type":"code","source":["@d2l.add_to_class(LinearRegressionScratch) #save\n","def loss(self, y_hat, y):\n","  l = (y_hat - y) ** 2 / 2\n","  return l.mean()"],"metadata":{"id":"6N6MiPJYopOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.4.3 Defining the Optimization Algorithm"],"metadata":{"id":"zCR7YD1tpHHf"}},{"cell_type":"code","source":["class SGD(d2l.HyperParameters):\n","  \"\"\"Minibatch stochastic gradient descent.\"\"\"\n","  def __init__(self, params, lr):\n","    self.save_hyperparameters()\n","\n","  def step(self):\n","    for param in self.params:\n","      param -= self.lr * param.grad\n","\n","  def zero_grad(self):\n","    for param in self.params:\n","      if param.grad is not None:\n","        param.grad.zero_()"],"metadata":{"id":"3VcYkWtMpE6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(LinearRegressionScratch) #save\n","def configure_optimizers(self):\n","  return SGD([self.w, self.b], self.lr)"],"metadata":{"id":"-YvD64xrqLXP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.4.4 Training"],"metadata":{"id":"51acYPY8qkRj"}},{"cell_type":"code","source":["@d2l.add_to_class(d2l.Trainer) #save\n","def prepare_batch(self, batch):\n","  return batch\n","\n","@d2l.add_to_class(d2l.Trainer) #save\n","def fit_epoch(self):\n","  self.model.train()\n","  for batch in self.train_dataloader:\n","    loss = self.model.training_step(self.prepare_batch(batch))\n","    self.optim.zero_grad()\n","    with torch.no_grad():\n","      loss.backward()\n","      if self.gradient_clip_val > 0:\n","        self.clip_gradients(self.gradient_clip_val, self.model)\n","      self.optim.step()\n","    self.train_batch_idx += 1\n","  if self.val_dataloader is None:\n","    return\n","  self.model.eval()\n","  for batch in self.val_dataloader:\n","    with torch.no_grad():\n","      self.model.validation_step(self.prepare_batch(batch))\n","    self.val_batch_idx += 1"],"metadata":{"id":"Dt5-yFpnqYcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = LinearRegressionScratch(2, lr=0.03)\n","data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n","trainer = d2l.Trainer(max_epochs=3)\n","trainer.fit(model, data)"],"metadata":{"id":"bxBHejMBr6jy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  print(f'error in estimating w: {data.w - model.w.reshape(data.w.shape)}')\n","  print(f'error in estimating b: {data.b - model.b}')"],"metadata":{"id":"VmuRB6assXpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Discussion & Review 3.4"],"metadata":{"id":"MNDnYiPnttSX"}},{"cell_type":"markdown","source":["###3.4.4\n","\n","- With the model, loss function, and optimizer components defined, we can implement the main training loop. This loop orchestrates the training process by iterating over batches of data, calculating the loss, computing gradients, and updating the model’s parameters.\n","\n","The Training Loop Steps\n","\n","1. Initialize Parameters: Set the initial weights and biases.\n","2. Repeat for Each Epoch:\n","  - Compute the gradient of the loss with respect to the parameters.\n","  - Update the parameters using the optimizer.\n","  - Optionally, measure performance on a validation dataset.\n","\n","- The training loop processes all training data divided into minibatches. This implementation is encapsulated within the Trainer class."],"metadata":{"id":"r86-r4HMG4rU"}},{"cell_type":"markdown","source":["##4.1 Softmax Regression"],"metadata":{"id":"Mf8YDaRxJa_F"}},{"cell_type":"markdown","source":["Information Theory Basics\n","\n","Information theory provides a framework for understanding how to encode, transmit, and manipulate data, which is essential for many deep learning applications.\n","\n","####4.1.3.1 Entropy\n","\n","- Definition: Entropy quantifies the amount of uncertainty or information present in a data distribution.\n","- Encoding Limit: According to Shannon’s theorem, a minimum of “nats” is required to encode data from a distribution. A \"nat\" is the natural logarithm equivalent of a \"bit\" in base 2.\n","\n","####4.1.3.2. Surprisal\n","\n","- Concept: Surprisal measures the unexpectedness of an event based on its probability. Events with lower probabilities yield higher surprisal values.\n","\n","####4.1.3.3. Cross-Entropy Revisited\n","\n","- Definition: Cross-entropy measures the expected surprisal when an observer employs subjective probabilities to predict data that follows a true probability distribution.\n","\n","- Relation to Entropy: The minimum cross-entropy occurs when subjective probabilities perfectly align with true probabilities.\n","\n","- Objective: In classification tasks, cross-entropy serves dual purposes:\n","\n","    (i) maximizing the likelihood of observed data\n","\n","    (ii) minimizing the surprisal (and thereby the bits needed) for effectively communicating the labels."],"metadata":{"id":"Y--artPVJnQu"}},{"cell_type":"markdown","source":["##4.2 The Image Classification Dataset"],"metadata":{"id":"PJnxbbj0xgOX"}},{"cell_type":"code","source":["%matplotlib inline\n","import time\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from d2l import torch as d2l\n","\n","d2l.use_svg_display()"],"metadata":{"id":"JQnnwKlJxmio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.2.1 Loading the Dataset"],"metadata":{"id":"ZosrTy5Txlw3"}},{"cell_type":"code","source":["class FashionMNIST(d2l.DataModule): #save\n","  \"\"\"The Fashion-MNIST dataset.\"\"\"\n","  def __init__(self, batch_size = 64, resize = (28, 28)):\n","    super().__init__()\n","    self.save_hyperparameters()\n","    trans = transforms.Compose([transforms.Resize(resize),\n","                                transforms.ToTensor()])\n","    self.train = torchvision.datasets.FashionMNIST(\n","     root = self.root, train = True, transform =trans, download =True)\n","    self.val = torchvision.datasets.FashionMNIST(\n","        root =self.root, train = False, transform = trans, download =True)"],"metadata":{"id":"88Oqhf3Mx3Kf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = FashionMNIST(resize=(32, 32))\n","len(data.train), len(data.val)"],"metadata":{"id":"ChGHDlJuy2QF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.train[0][0].shape"],"metadata":{"id":"tCzW9xqazBIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(FashionMNIST) #save\n","def text_labels(self, indices):\n","  \"\"\"Return text labels.\"\"\"\n","  labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n","            'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n","  return [labels[int(i)] for i in indices]"],"metadata":{"id":"X4ZTHDZTzNQi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.2.2 Reading a Minibatch"],"metadata":{"id":"An9EKdbW7BxH"}},{"cell_type":"code","source":["@d2l.add_to_class(FashionMNIST) #save\n","def get_dataloader(self, train):\n","  data = self.train if train else self.val\n","  return torch.utils.data.DataLoader(data, self.batch_size, shuffle = train,\n","                                     num_workers = self.num_workers)"],"metadata":{"id":"vzyD_5UxzuRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y = next(iter(data.train_dataloader()))\n","print(X.shape, X.dtype, y.shape, y.dtype)"],"metadata":{"id":"fIyKh_aG8Gne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tic = time.time()\n","for X, y in data.train_dataloader():\n","  continue\n","f'{time.time() - tic:.2f} sec'"],"metadata":{"id":"V4tifMmn87rn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.2.3 Visualization"],"metadata":{"id":"D1WbtglV9aYp"}},{"cell_type":"code","source":["def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #save\n","  \"\"\"Plot a list of images.\"\"\"\n","  raise NotImplementedError"],"metadata":{"id":"mGJzF7Ch9MoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(FashionMNIST) #save\n","def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n","  X, y = batch\n","  if not labels:\n","    labels = self.text_labels(y)\n","  d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\n","batch = next(iter(data.val_dataloader()))\n","data.visualize(batch)"],"metadata":{"id":"CkxlInKi90S-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##4.3 The Base Classification Model"],"metadata":{"id":"XUaoasHZ_YyV"}},{"cell_type":"markdown","source":["##4.3.1 The Classifier Class"],"metadata":{"id":"CfjDV5QT_d_s"}},{"cell_type":"code","source":["class Classifier(d2l.Module):\n","  \"\"\"The base class of classification models.\"\"\"\n","  def validation_step(self, batch):\n","    Y_hat = self(*batch[:-1])\n","    self.plot('loss', self.loss(Y_hat, batch[-1], train=False))\n","    self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)"],"metadata":{"id":"6Cw5wE8k-XI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(d2l.Module) #save\n","def configure_optimizers(self):\n","  return torch.optim.SGD(self.parameters(), lr = self.lr)"],"metadata":{"id":"vN2ZTXjDBA7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.3.2 Accuracy"],"metadata":{"id":"tdEDsiz_Bj4U"}},{"cell_type":"code","source":["@d2l.add_to_class(Classifier)\n","def accuracy(self, Y_hat, Y, averaged=True):\n","  \"\"\"Compute the number of correct predictions.\"\"\"\n","  Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n","  preds = Y_hat.argmax(axis = 1).type(torch.float32)\n","  return compare.mean() if averaged else compare\n"],"metadata":{"id":"kgEmxEA_BgPK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##4.4 Softmax Regression Implementatio from Scratch"],"metadata":{"id":"K6HDAXullALD"}},{"cell_type":"markdown","source":["###4.4.1 The Softmax"],"metadata":{"id":"IRKKq32wlFx_"}},{"cell_type":"code","source":["X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n","X.sum(0, keepdims=True), X.sum(1, keepdims=True)"],"metadata":{"id":"-qDi-WRMlFKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def softmax(X):\n","  X_exp = torch.exp(X)\n","  partition = X_exp.sum(1, keepdims=True)\n","  return X_exp / partition # The broadcasting mechanism is applied here"],"metadata":{"id":"Rs8nyTCXlczs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = torch.rand((2, 5))\n","X_prob = softmax(X)\n","X_prob, X_prob.sum(1)"],"metadata":{"id":"kj3O5fDAlwOP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.4.2 The Model"],"metadata":{"id":"nfOf4Lfkl8-q"}},{"cell_type":"code","source":["class SoftmaxRegressionScratch(d2l.Classifier):\n","  def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):\n","    super().__init__()\n","    self.save_hyperparameters()\n","    self.W = torch.normal(0, sigma, size = (num_inputs, num_outputs),\n","                         requires_grad = True)\n","    self.b = torch.zeros(num_outputs, requires_grad = True)\n","\n","  def parameters(self):\n","    return [self.W, self.b]"],"metadata":{"id":"MMr_KVGKl5di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(SoftmaxRegressionScratch)\n","def forward(self, X):\n","  X = X.reshape((-1, self.W.shape[0]))\n","  return softmax(torch.matmul(X, self.W) + self.b)"],"metadata":{"id":"2_XS0FZmmxfA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.4.3 The Cross-Entropy Loss"],"metadata":{"id":"bhylC1DvnQnh"}},{"cell_type":"code","source":["y = torch.tensor([0, 2])\n","y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2,0.5]])\n","y_hat[[0, 1], y]"],"metadata":{"id":"vjKOpCConCHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cross_entropy(y_hat, y):\n","  return -torch.log(y_hat[list(range(len(y_hat))), y]).mean()\n","\n","cross_entropy(y_hat, y)"],"metadata":{"id":"BT1UQQijnkMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(SoftmaxRegressionScratch)\n","def loss(self, y_hat, y):\n","  return cross_entropy(y_hat, y)"],"metadata":{"id":"TlJFOpPKnzYT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.4.4 Training"],"metadata":{"id":"RjmbK2VvoDKh"}},{"cell_type":"code","source":["data = d2l.FashionMNIST(batch_size=256)\n","model = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)\n","trainer = d2l.Trainer(max_epochs=10)\n","trainer.fit(model, data)"],"metadata":{"id":"TVYXMO_VoAp5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4.4.5 Prediction"],"metadata":{"id":"62Z4cuPwr58d"}},{"cell_type":"code","source":["X, y = next(iter(data.val_dataloader()))\n","preds = model(X).argmax(axis = 1)\n","preds.shape"],"metadata":{"id":"746l4-7uoXLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wrong = preds.type(y.dtype) != y\n","X, y, preds = X[wrong], y[wrong], preds[wrong]\n","labels = [a+'\\n' + b for a, b in zip(\n","    data.text_labels(y), data.text_labels(preds))]\n","data.visualize([X, y], labels = labels)"],"metadata":{"id":"Mha2mFOPsLS3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Discussion and Exercises 4.4"],"metadata":{"id":"w4Qf6LpFs1FA"}},{"cell_type":"markdown","source":["####4.4.1.\n","\n","The softmax function converts logits (raw prediction scores) into probabilities through three main steps:\n","\n","1. Exponentiation: Compute the exponential of each element.\n","2. Normalization: Calculate the sum of the exponentials for each row to determine the normalization constant.\n","3. Division: Divide each element by its row’s normalization constant, ensuring the results sum to 1."],"metadata":{"id":"qtDrN0NZKmTG"}},{"cell_type":"markdown","source":["##5.1 Multilayer Perceptrons"],"metadata":{"id":"3k8HhRaMtDBE"}},{"cell_type":"markdown","source":["###5.1.2 Activation Functions"],"metadata":{"id":"Pn5swT6ZtMJG"}},{"cell_type":"markdown","source":["####5.1.2.1 ReLU Function"],"metadata":{"id":"CZ9of29BtRKZ"}},{"cell_type":"code","source":["x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n","y = torch.relu(x)\n","d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize = (5, 2.5))"],"metadata":{"id":"ohK_HQmAtGPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.backward(torch.ones_like(x), retain_graph = True)\n","d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize = (5, 2.5))"],"metadata":{"id":"3vO1ge7YtocB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####5.1.2.2 Sigmoid Function"],"metadata":{"id":"oLRf7NMFt-MJ"}},{"cell_type":"code","source":["y = torch.sigmoid(x)\n","d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize = (5, 2.5))"],"metadata":{"id":"C-y7eKdxt7qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear out previous gradients\n","x.grad.data.zero_()\n","y.backward(torch.ones_like(x), retain_graph = True)\n","d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize = (5,2.5))"],"metadata":{"id":"vbHknga3uOxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####5.1.2.3 Tanh Function"],"metadata":{"id":"jCfXmY-KupgF"}},{"cell_type":"code","source":["y = torch.tanh(x)\n","d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))"],"metadata":{"id":"R4fLIcTnunVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear out previous gradients\n","x.grad.data.zero_()\n","y.backward(torch.ones_like(x), retain_graph = True)\n","d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize = (5, 2.5))"],"metadata":{"id":"xYqlL2GDvCGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Discussion and Exercises 5.1"],"metadata":{"id":"IOWpFIBPvaaK"}},{"cell_type":"markdown","source":["####5.1.1.1.\n","Linear models, such as softmax regression, map inputs to outputs via a single affine transformation, which limits their ability to capture complex relationships.\n","\n","- Monotonicity: Linear models assume that increases in input features correspond to consistent increases or decreases in output. This assumption can fail in many contexts:\n","  - Income Prediction: A higher income might increase the probability of loan repayment, but the relationship is not necessarily linear across all income ranges.\n","  - Health Prediction: For body temperature, higher temperatures can indicate greater health risks above a certain threshold, while lower temperatures can also indicate risks.\n","  - Image Classification: Relying solely on pixel brightness to distinguish between categories like cats and dogs ignores the complex interplay of pixel values."],"metadata":{"id":"aHMIOZ-dK8ht"}},{"cell_type":"markdown","source":["####5.1.1.2\n","- MLP Structure: An MLP consists of:\n","\n","  - An input layer (receives the features).\n","  - One or more hidden layers (process inputs and learn representations).\n","  - An output layer (produces final predictions).\n","\n","In a simple MLP:\n","\n","- The input layer might have 4 features.\n","- The hidden layer could have 5 neurons (hidden units).\n","- The output layer may have 3 output classes."],"metadata":{"id":"JVBDTqTPLLhm"}},{"cell_type":"markdown","source":["####5.1.1.3\n","\n","For an MLP with one hidden layer:\n","\n","- The hidden layer produces hidden representations, calculated as an affine transformation of the input.\n","- The final layer (output layer) applies another affine transformation to the hidden representations.\n","\n","1. Hidden Layer Output:\n","\n","  $H = f(W_h X + b_h)$\n","\n","2. Output Layer:\n","  \n","  $\\hat{Y} = W_o H + b_o$"],"metadata":{"id":"6tCfDrmcLesP"}},{"cell_type":"markdown","source":["###5.1.2\n","\n","A commonly used activation function is the ReLU (Rectified Linear Unit), defined as:\n","\n","  $f(x) = \\max(0, x)$\n","\n","Key Characteristics:\n","\n","- Nonlinear Transformation: ReLU retains only positive values and sets negative values to zero, effectively introducing nonlinearity into the model.\n","\n","- Computational Efficiency: The piecewise linear nature of ReLU allows for efficient computation, as it requires only a simple comparison operation.\n","\n","- Sparsity: By zeroing out negative values, ReLU can lead to sparsity in the network, which may improve efficiency and performance.\n","\n","\n","Its derivation:\n","\n","- For $x > 0$: he output of the ReLU function is equal to $x$, so the derivative is:\n","  $f'(x)=1$\n","- For $x<0$: The output is 0, thus the derivative is:\n","  $f'(x)=0$\n","- For $x=0$: The function is not differentiable at this point. By convention, we typically set:\n","  $f'(0)=0$"],"metadata":{"id":"xy9uA8YgMDFj"}},{"cell_type":"markdown","source":["####5.1.2.1\n","- The ReLU (Rectified Linear Unit) activation function is widely used in deep learning due to its well-behaved derivatives. It outputs either zero (for negative inputs) or the input itself (for positive inputs).\n","\n","- The Parameterized ReLU (pReLU) addresses a limitation of the standard ReLU, which can lead to the \"dying ReLU\" problem where neurons become inactive. The pReLU introduces a small linear term for negative inputs:\n","\n","  $f(x) =\n","\\begin{cases}\n","x & \\text{if } x > 0 \\\\\n","\\alpha x & \\text{if } x \\leq 0\n","\\end{cases}\n","$\n"],"metadata":{"id":"870ZgyWTNe_y"}},{"cell_type":"markdown","source":["####5.1..2\n","The Sigmoid function is defined as:\n","  $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n","\n","This function transforms any input $x$ from the real line $(-\\infty, +\\infty)$ to the range $(0,1)$, making it especially useful for binary classification tasks. Key points about the sigmoid function include:\n","\n","- Squashing Function: The sigmoid squashes inputs to the range (0, 1), interpretable as probabilities.\n","- Historical Context: The sigmoid function gained popularity as it approximated biological neurons that fired (outputting 1) or did not fire (outputting 0).\n","\n","\n"],"metadata":{"id":"nANQD6lKN8vA"}},{"cell_type":"markdown","source":["Derivative of the Sigmoid Function\n","\n","The derivative of the sigmoid function can be expressed mathematically as:\n","\n","$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$\n","\n","$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n","\n","$\\sigma(0) = \\frac{1}{1 + e^0} = \\frac{1}{2}$\n","\n","$\\sigma'(0) = \\frac{1}{2} \\cdot \\left(1 - \\frac{1}{2}\\right) = \\frac{1}{2} \\cdot \\frac{1}{2} = 0.25$"],"metadata":{"id":"xNGutUHMO2Fg"}},{"cell_type":"markdown","source":["Tanh Function\n","\n","The hyperbolic tangent (tanh) function is a commonly used activation function in neural networks, transforming input values into a range between -1 and 1. It is mathematically defined as:\n","\n","$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n","\n","Properties of the Tanh Function\n","\n","1. Range: The output of the tanh function lies in the interval $(-1,1)$. This allows it to produce both positive and negative values, effectively centering the data around zero and often facilitating faster convergence during training.\n","\n","2. Symmetry: The tanh function is odd, exhibiting point symmetry about the origin:\n","\n","  $\\tanh(-x) = -\\tanh(x)$\n","\n","3. Behavior Near Zero: Similar to the sigmoid function, the tanh function approaches a linear transformation near $x=0$:\n","  - For small values of $x$:\n","  $\\tanh(x) \\approx x$\n","\n","4. Derivative: The derivative of the tanh function can be expressed as:\n","\n","  $\\tanh'(x) = 1 - \\tanh^2(x)$\n"],"metadata":{"id":"OyC8BT4SPfNI"}},{"cell_type":"markdown","source":["Derivative of the Tanh Function\n","\n","The derivative of the hyperbolic tangent (tanh) function is essential for understanding its behavior during the training of neural networks. The mathematical expression for the derivative is:\n","\n","  $\\tanh'(x) = 1 - \\tanh^2(x)$\n","\n","Properties of the Derivative\n","\n","Maximum at Zero: The derivative reaches its maximum value of 1 when $x=0$, indicating that the function is most sensitive to changes in input around this point.\n","\n","Vanishing Gradients: As the input $x$ moves away from 0 in either direction (positive or negative), the derivative approaches 0. This phenomenon, known as the vanishing gradient problem, can impede the training of deep networks, as gradients become too small to effect significant updates during backpropagation."],"metadata":{"id":"rIAb52GRQiXu"}},{"cell_type":"markdown","source":["##5.2 Implementation of Multilayer Perceptrons"],"metadata":{"id":"QP9OOTRsxzyx"}},{"cell_type":"markdown","source":["###5.2.1 Implementation from Scratch"],"metadata":{"id":"DhRRQ2gwx5ET"}},{"cell_type":"markdown","source":["####5.2.1.1 Initializing Model Parameters"],"metadata":{"id":"o3EXB19wx9E4"}},{"cell_type":"code","source":["class MLPScratch(d2l.Classifier):\n","  def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma = 0.01):\n","    super().__init__()\n","    self.save_hyperparameters()\n","    self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n","    self.b1 = nn.Parameter(torch.zeros(num_hiddens))\n","    self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n","    self.b2 = nn.Parameter(torch.zeros(num_outputs))"],"metadata":{"id":"_PqG3Nyax4bJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####5.2.1.2 Model"],"metadata":{"id":"mUJTGJZOy828"}},{"cell_type":"code","source":["def relu(X):\n","  a = torch.zeros_like(X)\n","  return torch.max(X, a)"],"metadata":{"id":"Zq_kKhm6y7Uu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@d2l.add_to_class(MLPScratch)\n","def forward(self, X):\n","  X = X.reshape((-1, self.num_inputs))\n","  H = relu(torch.matmul(X, self.W1) + self.b1)\n","  return torch.matmul(H, self.W2) + self.b2"],"metadata":{"id":"2mOnl_1RzE_d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####5.2.1.3 Training"],"metadata":{"id":"ZkHCTORVzg3f"}},{"cell_type":"code","source":["model = MLPScratch(num_inputs = 784, num_outputs = 10, num_hiddens = 256, lr = 0.1)\n","data = d2l.FashionMNIST(batch_size = 256)\n","trainer = d2l.Trainer(max_epochs = 10)\n","trainer.fit(model, data)"],"metadata":{"id":"lNlbVFCpzeA_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###5.2.2 Concise Implementation"],"metadata":{"id":"kiXMaoxvz_hL"}},{"cell_type":"markdown","source":["####5.2.2.1 Model"],"metadata":{"id":"LQ4fLwM60DLi"}},{"cell_type":"code","source":["class MLP(d2l.Classifier):\n","  def __init__(self, num_outputs, num_hiddens, lr):\n","    super().__init__()\n","    self.save_hyperparameters()\n","    self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),\n","                             nn.ReLU(), nn.LazyLinear(num_outputs))"],"metadata":{"id":"NKVkz_XCz2hD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####5.2.2.2 Training"],"metadata":{"id":"jZzupofH0llJ"}},{"cell_type":"code","source":["model = MLP(num_outputs = 10, num_hiddens = 256, lr = 0.1)\n","trainer.fit(model, data)"],"metadata":{"id":"Rlkh7mX-0jGi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Discussion & Review 5.3"],"metadata":{"id":"sGz-l1Dj0zfZ"}},{"cell_type":"markdown","source":["###5.3.1. Forward Propagation\n","\n","Forward propagation refers to the process of calculating and storing intermediate variables and outputs as data flows from the input layer through to the output layer. Here’s a breakdown of the forward propagation in a one-hidden-layer MLP:\n","\n","1. Input and Hidden Layer Computations:\n","  - Given an input vector $x$ and the weight matrix $W_1$ for the hidden layer (excluding bias for simplicity), the intermediate variable for the hidden layer is computed as:\n","\n","  $z = W_1^\\top x$\n","\n","  - After applying an activation function (e.g., ReLU), the hidden activation vector $h$ is:\n","\n","  $h = ReLU(z)$\n","\n","2. Output Layer Computation:\n","\n","  Assuming the output layer has weight matrix $W2$, the output variable $y$ can be computed as:\n","\n","  $y = W_2^\\top h$\n","\n","3. Loss Calculation:\n","\n","  If we denote the loss function as $L$ and the true label as $y_{true}$, the loss for a single data example is given by:\n","\n","  $Loss=L(y, y_{true})$\n","\n","\n","4. Regularization:\n","\n","- Incorporating weight decay regularization with a hyperparameter $λ$, the regularization term is:\n","\n","  $\\text{Regularization} = \\frac{1}{2} \\lambda \\|W_1\\|_F^2 + \\frac{1}{2} \\lambda \\|W_2\\|_F^2$\n","\n","- The overall regularized loss for a given data example becomes:\n","  $Objective=Loss+Regularization$\n","\n"],"metadata":{"id":"SW8bmppjRDWj"}},{"cell_type":"markdown","source":["####5.3.3. Backpropagation\n","\n","Backpropagation is the algorithm used to compute gradients for all parameters in the network. It involves traversing the network in reverse order (from output back to input), applying the chain rule of calculus.\n","\n","1. Calculate Gradients of the Loss:\n","\n","  Start by computing the gradient of the objective function $J$ with respect to the loss and regularization terms:\n","\n","  $\\frac{\\partial \\text{Loss}}{\\partial J} \\quad \\text{and} \\quad \\frac{\\partial \\text{Regularization}}{\\partial J}$\n","\n","2. Gradient of Output Layer:\n","\n","  Compute the gradient of the objective with respect to the output layer variable:\n","\n","  $\\frac{\\partial y}{\\partial J} = \\frac{\\partial J}{\\partial \\text{Loss}} \\cdot \\frac{\\partial \\text{Loss}}{\\partial y}$\n","\n","3. Gradient of Parameters:\n","\n","- Compute gradients with respect to the parameters using the chain rule:\n","\n","  $\\frac{\\partial W_2}{\\partial J} = \\frac{\\partial y}{\\partial J} \\cdot \\mathbf{h}^\\top$\n","\n","- For the hidden layer:\n","  \n","  $\\frac{\\partial \\mathbf{h}}{\\partial J} = \\frac{\\partial y}{\\partial J} \\cdot W_2$\n","\n","4. Hidden Layer Gradients:\n","\n","  Compute the gradient of the hidden activation and then the parameters of the hidden layer:\n","\n","  $\\frac{\\partial z}{\\partial J} = \\frac{\\partial h}{\\partial J} \\cdot \\text{ReLU}'(z)$\n","\n","  $\\frac{\\partial J}{\\partial W_1} = \\frac{\\partial z}{\\partial J} \\cdot x^\\top$\n","\n","5. Final Gradients:\n","\n","  Finally, compute the gradients of the regularization term with respect to both parameters:\n","\n","  $\\frac{\\partial W_1}{\\partial J} + \\lambda W_1 \\quad \\text{and} \\quad \\frac{\\partial W_2}{\\partial J} + \\lambda W_2$\n"],"metadata":{"id":"mJTeBgphSvNw"}},{"cell_type":"markdown","source":["###5.3.4. Training Neural Networks\n","\n","Interdependence of Forward and Backward Propagation\n","\n","1. Forward Propagation:\n","\n","  - During forward propagation, we traverse the computational graph from input to output, calculating all intermediate variables and the final output.\n","  - This phase includes the computations through each layer and the calculation of the regularization term, which relies on the current model parameters.\n","\n","2. Backward Propagation:\n","\n","  - In the backward pass, we calculate gradients using the chain rule. The gradients for model parameters are directly influenced by the outputs from the forward pass, particularly the hidden layer outputs and the computed loss.\n","\n","The Training Process\n","\n","1. Initialization: Begin with randomly initialized model parameters.\n","\n","2. Forward Propagation:\n","\n","  - Compute outputs based on the current parameters and input data.\n","  - Calculate the loss, incorporating any regularization terms.\n","\n","3. Backward Propagation:\n","\n","  - Compute gradients with respect to the loss using the stored intermediate values from the forward pass.\n","  - Update the model parameters using an optimization algorithm (e.g., stochastic gradient descent).\n","\n","4. Iteration:\n","\n","  - Repeat the forward and backward passes for multiple epochs until the model converges or achieves satisfactory performance.\n"],"metadata":{"id":"I6PpI7VRU_og"}}]}