{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNkrsX4TQW689hmNWehPvqe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##3.1 Linear Regression"],"metadata":{"id":"xbT9SGzPekeG"}},{"cell_type":"code","source":["pip install d2l==1.0.3"],"metadata":{"collapsed":true,"id":"-5eMFtO6fBCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2p9o7IDeP2c"},"outputs":[],"source":["%matplotlib inline\n","import math\n","import time\n","import numpy as np\n","import torch\n","from d2l import torch as d2l"]},{"cell_type":"markdown","source":["###3.1.2 Vectorization for Speed"],"metadata":{"id":"GFmbtGDleoie"}},{"cell_type":"code","source":["n = 10000\n","a = torch.ones(n)\n","b = torch.ones(n)"],"metadata":{"id":"KWrQbOo3fnJT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c = torch.zeros(n)\n","t = time.time()\n","for i in range(n):\n","  c[i] = a[i] + b[i]\n","f'{time.time() - t:.5f} sec'"],"metadata":{"id":"h8iPHBdGiD72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = time.time()\n","d = a + b\n","f'{time.time() - t:.5f} sec'"],"metadata":{"id":"JKuU78tZiVO7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3.1.3 The Normal Distribution and Squared Loss"],"metadata":{"id":"Kc7O3Umfil5w"}},{"cell_type":"code","source":["def normal(x, mu, sigma):\n","  p = 1 / math.sqrt(2 * math.pi * sigma ** 2)\n","  return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)"],"metadata":{"id":"5okhG8OkigHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use NumPy again for visualization\n","x = np.arange(-7, 7, 0.01)\n","\n","# Mean and standard deviation pairs\n","params = [(0, 1), (0, 2), (3, 1)]\n","d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel = 'x',\n","         ylabel='p(x)', figsize=(4.5, 2.5),\n","         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])"],"metadata":{"id":"aqixOJtijL6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Discussion and Review 3.1"],"metadata":{"id":"xY0E1R5ukgH2"}},{"cell_type":"markdown","source":["Key Concepts\n","\n","- Regression Problems: Focus on predicting continuous numerical values, such as house prices, lengths of hospital stays, and stock prices.\n","- Training Dataset: Contains features (inputs) and labels (targets) used to teach the model.\n","- Features: Variables influencing predictions (e.g., area, age of a house).\n","- Labels/Targets: Values that need to be predicted (e.g., house prices)."],"metadata":{"id":"io89Gv8B5xqD"}},{"cell_type":"markdown","source":["###3.1.1\n","\n","- Definition: Linear regression is a foundational method for regression tasks, based on the assumption of a linear relationship between the target and features.\n","\n","- Model Equation:\n","  $y = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b$\n","\n","  - $w_1$ and $w_2$ : Weights corresponding to features.\n","\n","  - $b$ : Bias term to fit data even when features are zero.\n","\n","- Noise: Real-world data contains noise, often assumed to follow a Gaussian distribution."],"metadata":{"id":"2HsBpend6AIt"}},{"cell_type":"markdown","source":["####3.1.1.1\n","\n","Model Representation\n","- Linear models express the target $y$ as a sum of weighted features plus bias:\n","\n","  $\\hat{y} = w_1 \\cdot \\text{(area)} + w_2 \\cdot \\text{(age)} + b$\n","\n","- Weights: Determine each feature's contribution to predictions.\n","\n","- Bias: The model’s output when all features are zero.\n","\n","- Affine Transformation: Linear regression involves a linear transformation (weighted sum) plus a translation (bias).\n","\n","Compact Representation\n","- High-Dimensional Data: Using vector and matrix notation simplifies representation:\n","  - Single Example Prediction:\n","  $\\hat{y} = w^\\top x + b$\n","  - Multiple Examples Prediction:\n","  $\\hat{y} = Xw + b$\n","  - $x$: Feature vector, $w$: Weight vector, $X$: Design matrix of all examples.\n","\n","Goal of Linear Regression\n","\n","- Objective: Minimize prediction error by optimizing weight vector $w$ and bias $b$.\n","- Error Sources: Measurement inaccuracies and inherent noise in data.\n","- Noise Term: Essential to include in the model to account for discrepancies.\n","\n","Next Steps in Linear Regression\n","\n","- Model Quality: Establish a metric to evaluate prediction quality, often through loss functions.\n","- Optimization: Implement algorithms (e.g., gradient descent) to iteratively refine model parameters to reduce prediction errors."],"metadata":{"id":"omkH2VwG6xu7"}},{"cell_type":"markdown","source":["####3.1.1.2\n","\n","- Loss Function: Quantifies how well the model’s predictions match actual targets, with lower values indicating better performance.\n","  - Squared Error: Commonly used loss function for regression:\n","  $L(\\hat{y}, y) = \\frac{1}{2} (\\hat{y} - y)^2$\n","    - Penalizes larger errors more severely due to the quadratic nature.\n","    - The constant $\\frac{1}{2}$ simplifies the derivative calculation.\n","\n","- Total Loss: For the entire dataset, the total loss is averaged or summed:\n","\n","  $\\text{Total Loss} = \\sum_{i=1}^{n} L(\\hat{y}^{(i)}, y^{(i)})$\n","\n","    - Objective: Minimize this total loss by adjusting model parameters (weights and bias)."],"metadata":{"id":"zronJeknBD0v"}},{"cell_type":"markdown","source":["####3.1.1.3\n","\n","- Analytical Solution: Linear regression offers a mathematical method to find optimal parameters.\n","\n","  - By including a column of 1s for the bias, the normal equation can be derived:\n","  $w = (X^\\top X)^{-1} X^\\top y$\n","  - This solution is unique if the design matrix $X$ has full rank (i.e., its columns are linearly independent).\n","\n","- Complex Models: Analytical solutions are rare in deep learning due to increased model complexity, necessitating iterative optimization techniques."],"metadata":{"id":"_WgUSBHKBwNe"}},{"cell_type":"markdown","source":["####3.1.1.4\n","\n","- Gradient Descent: An optimization technique for updating model parameters to reduce loss iteratively.\n","\n","- Full-batch Gradient Descent: Utilizes all data points to compute gradients but may be slow for large datasets.\n","\n","- Stochastic Gradient Descent (SGD): Updates parameters using a single randomly selected sample at each step, which is faster but less stable.\n","\n","- Minibatch SGD: A hybrid approach that uses small batches for updates, providing a balance of efficiency and stability:\n","  $w \\leftarrow w - \\eta \\cdot \\sum_{i=1}^{m} \\nabla L(\\hat{y}^{(i)}, y^{(i)})$\n","  - $m$: Minibatch size\n","  - $\\eta$: Learning rate, a small positive value that controls step size.\n","\n","- Hyperparameters: Learning rate and minibatch size are not learned during training but set beforehand, often tuned using methods like Bayesian optimization.\n","\n","- Stopping Criterion: Training concludes after a predetermined number of iterations or upon meeting a specific condition (e.g., loss stabilization)."],"metadata":{"id":"_J6RZUrBCSG8"}},{"cell_type":"markdown","source":["####3.1.1.5\n","\n","- Making Predictions: The trained model can make predictions on unseen data:\n","  $\\hat{y} = w^\\top x + b$\n","  - This phase is referred to as inference in deep learning, although this term may lead to confusion with statistical inference.\n","- Generalization: The goal is not just to minimize training loss but to ensure the model performs well on unseen data, a concept known as generalization."],"metadata":{"id":"xIQbenB1C0sk"}},{"cell_type":"markdown","source":["###3.1.3\n","\n","- Squared Loss: There is a significant relationship between squared loss and the normal distribution.\n","\n","  - When using squared loss in linear regression, it is optimal under the assumption that the noise (the residuals between predicted and actual values) follows a normal distribution.\n","- Normal Distribution:\n","  - Characterized by a mean $\\mu$ and variance $\\sigma^2$, the probability density function (PDF) is expressed as:\n","\n","  $p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$\n","  - Often referred to as the Gaussian distribution, it is foundational in statistics and machine learning.\n","- Connection between Squared Loss and Normal Distribution:\n","\n","- In the context of linear regression, if the errors (or noise) are assumed to follow a normal distribution, minimizing the squared loss equates to maximizing the likelihood of the observed data under that distribution."],"metadata":{"id":"_p82jD7UDJym"}},{"cell_type":"markdown","source":["###3.1.3\n","\n","- When fitting a linear regression model, we assume that the observed data points stem from a deterministic relationship with some added noise, which is typically modeled as following a normal distribution. This leads us to express the relationship as:\n","\n","  $y = w^\\top x + b + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)$\n","\n","- This means that the target variable $y$ is modeled as a linear combination of input features $x$, plus some normally distributed noise $\\epsilon$ .\n","\n","Likelihood of Observing Data\n","\n","- The likelihood of observing a specific target value $y$ for a given input $x$ is given by:\n","\n","  $P(y \\mid x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y - (w^\\top x + b))^2}{2\\sigma^2}\\right)$"],"metadata":{"id":"kY31SleKDw4a"}},{"cell_type":"markdown","source":["####3.1.3.1\n","\n","The overall likelihood for the entire dataset, considering $n$ independent observations, is:\n","\n","  $L(w, b) = \\prod_{i=1}^{n} P(y_i \\mid x_i)$\n","\n","To optimize this likelihood, we can take the logarithm (log-likelihood), which simplifies the calculations. The log-likelihood for all observations is:\n","\n","  $\\log L(w, b) = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - (w^\\top x_i + b))^2$\n","\n","Ignoring constant terms that do not depend on $w$ and $b$, we focus on minimizing the mean squared error (MSE), which corresponds to the maximum likelihood estimation (MLE) under the assumption of Gaussian noise:\n","\n","  $\\min_{w, b} \\sum_{i=1}^{n} (y_i - (w^\\top x_i + b))^2$\n"],"metadata":{"id":"4VHqy21dEeue"}},{"cell_type":"markdown","source":["###3.1.4\n","\n","Despite its simplicity, linear regression can be interpreted as a basic form of a neural network:\n","\n","- Input Layer: Contains neurons representing each input feature $x_1, x_2, ..., x_d.$\n","\n","Output Layer: Produces a single output $\\hat y$, representing the model's prediction.\n","\n","In this neural network perspective:\n","\n","- There is a single output neuron.\n","- The input layer is fully connected to the output layer.\n","- There are no hidden layers.\n","\n","While this architecture is too simplistic for complex tasks, it lays the groundwork for more advanced neural network architectures."],"metadata":{"id":"Jvh-rnDyFIfT"}},{"cell_type":"markdown","source":["####3.1.4.1\n","\n"," A biological neuron includes:\n","\n","- Dendrites: Receive input signals.\n","- Nucleus: Processes the weighted inputs.\n","- Axon: Transmits the output signal to other neurons or actuators."],"metadata":{"id":"EPh6eiz8FrHm"}}]}