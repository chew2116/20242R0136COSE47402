{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "EcRq9SkkJx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "Wpioud6SNbni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "e9kqooPJiI8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pkdarabi/the-drug-name-detection-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "Jo9_nh9ZTsIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset paths\n",
        "data_dir = \"/root/.cache/kagglehub/datasets/pkdarabi/the-drug-name-detection-dataset/versions/1\""
      ],
      "metadata": {
        "id": "NvgUb2APU1-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "KYN0qYb1VQUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = datasets.ImageFolder(data_dir, transform=transform)"
      ],
      "metadata": {
        "id": "uj6bUz5GVTXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size, test_size]\n",
        ")"
      ],
      "metadata": {
        "id": "K-d5EGvpVnLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "jTQRs0BCVtXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to visualize a few images from the dataset\n",
        "def show_sample_images(loader, classes):\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "    images = images[:6]  # Show first 6 images\n",
        "    labels = labels[:6]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(15, 5))\n",
        "    for i in range(6):\n",
        "        img = images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5  # Denormalize\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(classes[labels[i]])\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize samples\n",
        "show_sample_images(train_loader, dataset.classes)\n"
      ],
      "metadata": {
        "id": "p8MWFRWzV9Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_dataset.dataset.transform = train_transform"
      ],
      "metadata": {
        "id": "WQNAVg1sWjJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get class distribution\n",
        "class_counts = Counter([label for _, label in dataset.samples])\n",
        "for cls, count in class_counts.items():\n",
        "    print(f\"{dataset.classes[cls]}: {count}\")\n"
      ],
      "metadata": {
        "id": "5tQzpucNWxyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "hq783pYUNCdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "XkCg6k1DNGv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 : Fine - Tune the CLIP Model\n",
        "from torch.optim import AdamW\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load pre-trained CLIP\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# for param in clip_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # Fine-tune only the classification layer or specific layers\n",
        "# for param in clip_model.text_projection.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# # Enable gradient computation for the vision encoder\n",
        "# for param in clip_model.vision_model.parameters():\n",
        "#     param.requires_grad = True  # Fine-tune all layers of the visual encoder\n",
        "\n",
        "# # If you also want to fine-tune the text encoder:\n",
        "# for param in clip_model.text_model.parameters():\n",
        "#     param.requires_grad = True  # Fine-tune all layers of the text encoder\n",
        "\n",
        "# Fine-tuning settings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model.to(device)\n",
        "optimizer = Adam(clip_model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "eE8Li3hwXFpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# De-normalize images for CLIPProcessor\n",
        "def denormalize(images, mean, std):\n",
        "    mean = torch.tensor(mean).view(1, 3, 1, 1).to(images.device)\n",
        "    std = torch.tensor(std).view(1, 3, 1, 1).to(images.device)\n",
        "    return images * std + mean\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    clip_model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # De-normalize images\n",
        "        images = denormalize(images, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "\n",
        "        # Convert images to PIL-compatible format using CLIPProcessor\n",
        "        inputs = clip_processor(images=images, return_tensors=\"pt\", do_rescale=False).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        image_features = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
        "        loss = criterion(image_features, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "UFT5I1CJaE5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "clip_model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # De-normalize images for CLIPProcessor\n",
        "        images = denormalize(images, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "\n",
        "        # Clamp pixel values to [0, 1]\n",
        "        images = torch.clamp(images, min=0.0, max=1.0)\n",
        "\n",
        "        # Process images\n",
        "        inputs = clip_processor(images=images.permute(0, 2, 3, 1).cpu(), return_tensors=\"pt\", do_rescale=False)\n",
        "        pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "\n",
        "        # Extract image features and classify\n",
        "        image_features = clip_model.get_image_features(pixel_values=pixel_values)\n",
        "\n",
        "        # Use a classification layer if added during training\n",
        "        preds = torch.argmax(image_features, dim=-1)\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Image Recognition Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFbG43Zyam0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.api import TTS\n",
        "\n",
        "# Initialize Tacotron2\n",
        "tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", gpu=torch.cuda.is_available())\n",
        "\n",
        "def generate_voice_output(medicine_name, output_path=\"medicine_output.wav\"):\n",
        "    tts.tts_to_file(medicine_name, file_path=output_path)\n",
        "    print(f\"Voice output saved as {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "generate_voice_output(\"Paracetamol\")\n"
      ],
      "metadata": {
        "id": "SRcDGd4IxRx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example listener scores\n",
        "listener_scores = {\n",
        "    \"listener_1\": [5, 4, 4],\n",
        "    \"listener_2\": [4, 4, 5],\n",
        "    \"listener_3\": [5, 5, 4]\n",
        "}\n",
        "\n",
        "# Calculate Mean Opinion Score\n",
        "mos_score = np.mean([np.mean(scores) for scores in listener_scores.values()])\n",
        "print(f\"Mean Opinion Score (MOS): {mos_score:.1f}\")"
      ],
      "metadata": {
        "id": "aiUTtddi0XZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gTTS"
      ],
      "metadata": {
        "id": "CNl52qRR5sEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS"
      ],
      "metadata": {
        "id": "s8vTyFkQ0jVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ResNet-50\n",
        "class ResNetClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetClassifier, self).__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# Initialize ResNet model\n",
        "# Use the dataset object to get the number of classes\n",
        "resnet_model = ResNetClassifier(num_classes=len(dataset.classes)).to(device)\n",
        "resnet_optimizer = torch.optim.Adam(resnet_model.parameters(), lr=1e-4)\n",
        "resnet_criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Q4RkJvDP3mPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for ResNet\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    resnet_model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = resnet_model(images)\n",
        "        loss = resnet_criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        resnet_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        resnet_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "umTXhEms4VnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "resnet_model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = resnet_model(images)\n",
        "        preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy_resnet = accuracy_score(all_labels, all_preds)\n",
        "print(f\"ResNet Image Recognition Accuracy: {accuracy_resnet * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ccQ1daJp5YUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "def basic_tts(medicine_name, output_path=\"basic_tts_output.wav\"):\n",
        "    tts = gTTS(text=medicine_name, lang='en')\n",
        "    tts.save(output_path)\n",
        "    print(f\"Basic TTS Voice Output Saved: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "basic_tts(\"Paracetamol\")"
      ],
      "metadata": {
        "id": "cZq0x7nR5t7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example listener scores\n",
        "listener_scores_resnet = {\n",
        "    \"listener_1\": [3, 4, 3],\n",
        "    \"listener_2\": [4, 4, 3],\n",
        "    \"listener_3\": [3, 4, 4]\n",
        "}\n",
        "\n",
        "# Calculate MOS\n",
        "mos_resnet = np.mean([np.mean(scores) for scores in listener_scores_resnet.values()])\n",
        "print(f\"Mean Opinion Score (MOS) for ResNet + Basic TTS: {mos_resnet:.1f}\")\n"
      ],
      "metadata": {
        "id": "W49XNzyR8H1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline SOTA\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "oBSZYJ-X5w8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "cFu4BanprhRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms, datasets\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize Vision Transformer (ViT)\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "vit_model = ViTForImageClassification.from_pretrained(model_name, num_labels=len(dataset.classes)).to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    vit_model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Ensure pixel values are in the correct range\n",
        "        images = torch.clamp(images, 0, 1)\n",
        "\n",
        "        # Process images with the feature extractor\n",
        "        inputs = feature_extractor(images=images.permute(0, 2, 3, 1).cpu(), return_tensors=\"pt\", do_rescale=False).to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = vit_model(**inputs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ltSr1SA88Thp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vit_model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the appropriate device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Rescale images to the expected range [0, 1]\n",
        "        images = torch.clamp((images + 1) / 2, 0, 1)  # Convert from [-1, 1] to [0, 1]\n",
        "\n",
        "        # Convert images for the ViT feature extractor\n",
        "        inputs = feature_extractor(images=images.permute(0, 2, 3, 1).cpu(), return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Pass through the ViT model\n",
        "        outputs = vit_model(**inputs)\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"ViT Image Recognition Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "OEaqljwM8y3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Voice Output with gTTS\n",
        "def generate_voice_output_gtts(text, output_path=\"output.mp3\"):\n",
        "    tts = gTTS(text=text, lang=\"en\")\n",
        "    tts.save(output_path)\n",
        "    print(f\"Voice output saved at {output_path}\")\n",
        "\n",
        "# Example Usage\n",
        "medicine_name = \"Paracetamol\"\n",
        "generate_voice_output_gtts(f\"The medicine name is {medicine_name}.\", output_path=\"medicine_name.mp3\")\n"
      ],
      "metadata": {
        "id": "TAV91vY2C80P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listener_scores = {\n",
        "    \"listener_1\": [4, 5, 4, 4],\n",
        "    \"listener_2\": [5, 4, 4, 5],\n",
        "    \"listener_3\": [4, 4, 5, 4]\n",
        "}\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Calculate MOS score\n",
        "mos_score = np.mean([np.mean(scores) for scores in listener_scores.values()])\n",
        "print(f\"Mean Opinion Score (MOS) for ViT: {mos_score:.1f}\")\n"
      ],
      "metadata": {
        "id": "v5qHpIb6YR-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = [\"CLIP + Tacotron2\", \"ResNet + Basic TTS\", \"Baseline (SOTA)\"]\n",
        "accuracy = [72.13, 56.28, 67.76]  # Replace with your accuracy values\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(models, accuracy, color=['blue', 'orange', 'green'])\n",
        "plt.ylabel(\"Image Recognition Accuracy (%)\")\n",
        "plt.title(\"Accuracy Comparison Across Models\")\n",
        "plt.ylim(0, 100)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DlawV6cGv1H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mos_scores = [4.4, 3.8, 4.3]  # Replace with your MOS values\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(models, mos_scores, color=['blue', 'orange', 'green'])\n",
        "plt.ylabel(\"Mean Opinion Score (MOS)\")\n",
        "plt.title(\"MOS Score Comparison Across Models\")\n",
        "plt.ylim(0, 5)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iSfXKd9bwAjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M8eEg_3RwD2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}